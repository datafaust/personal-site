[{"authors":["admin"],"categories":null,"content":"Welcome to my site, a place where I publish some of my work, personal projects and anything else that interests me.\nI am a creative and resourceful data engineer and software developer. Currently, I am the Director of Data Engineering and Analytics at the Taxi \u0026amp; Limousine Commission of the New York City. My work involves developing and automating big data processes, building and interacting with databases, creating apps and dashboards for TLC and public, as well as conducting high level research to empower impactful legislation in the area of city transportation. I am also a co-founder of VeryCool Studio - an app development, data analysis, and rapid prototyping company. In my spare time I work on data projects, repair motorcycles and make movies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/fausto-lopez/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/fausto-lopez/","section":"authors","summary":"Welcome to my site, a place where I publish some of my work, personal projects and anything else that interests me.\nI am a creative and resourceful data engineer and software developer.","tags":null,"title":"Fausto Lopez","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Fausto Lopez"],"categories":null,"content":"The Objective A little while ago I had covered how to load tlc trip records into clickhouse under WSL (Windows subsystems for Linux), on the idea that it could be useful for teams working behind microsoft systems. After working with clickhouse for some time, I\u0026rsquo;ve come to love how easy it is to use and how powerful the database technology is. For some time I\u0026rsquo;ve been toying with the idea of building out a multi-platform transportation app (doesn\u0026rsquo;t everyone) if anything to work on how I can integrate different data sets and practice some app construction.\nGet the Data When it comes to large data sets, citibike data is probably the next big thing after taxis so I figured I\u0026rsquo;d load that into clickhouse. I went ahead and cloned Todd Schneider\u0026rsquo;s repo; as a transportation guru he has both citibike and taxi repos holding ETL processes in postgresql. You\u0026rsquo;ll want to run the following to get the program running:\ngit clone https://github.com/toddwschneider/nyc-citibike-data.git cd nyc-citibike-data ./download_raw_data.sh ./initialize_database.sh \u0026amp;\u0026amp; ./import_trips.sh  This shouldn\u0026rsquo;t take too long, took about an hour for me to download everything and another hour for it to process. Once the data is in postgres you will want to extract .gz files, which are highly compressed and will be loaded into clickhouse. Under Todd\u0026rsquo;s schema you\u0026rsquo;ll see he creates a view with combine trip and geographical information:\nCREATE VIEW trips_and_stations AS ( SELECT t.*, ss.name AS start_station_name, ss.latitude AS start_station_latitude, ss.longitude AS start_station_longitude, ss.nyct2010_gid AS start_nyct2010_gid, ss.boroname AS start_boroname, ss.ntacode AS start_ntacode, ss.ntaname AS start_ntaname, ss.taxi_zone_gid AS start_taxi_zone_gid, ss.taxi_zone_name AS start_taxi_zone_name, es.name AS end_station_name, es.latitude AS end_station_latitude, es.longitude AS end_station_longitude, es.nyct2010_gid AS end_nyct2010_gid, es.boroname AS end_boroname, es.ntacode AS end_ntacode, es.ntaname AS end_ntaname, es.taxi_zone_gid AS end_taxi_zone_gid, es.taxi_zone_name AS end_taxi_zone_name FROM trips t INNER JOIN stations ss ON t.start_station_id = ss.id INNER JOIN stations es ON t.end_station_id = es.id );  We will want to pull more data than this table however, because we want to include weather data in our main table. You\u0026rsquo;ll want to run a left join against weather observations when pulling from postgres. But first, we will need to create a directory to pull the data and then provide the write permissions to write to that directory:\nmkdir -p /mnt/c/Users/yourcompname/nyc-citibike-data/trips $ sudo chown -R postgres:postgres \\ /home/yourcompname/nyc-citibike-data/trips  Now we can use copy to pull the data, I split the files into million chunk files:\nCOPY ( SELECT ts.trip_duration, ts.start_time, ts.stop_time, ts.start_station_id, ts.start_station_name, ts.start_station_latitude, ts.start_station_longitude, ts.end_station_id, ts.end_station_name, ts.end_station_latitude, ts.end_station_longitude, ts.bike_id, ts.user_type, ts.birth_year, ts.gender, ts.start_nyct2010_gid, ts.start_boroname, ts.start_ntacode, ts.start_ntaname, ts.start_taxi_zone_gid, ts.start_taxi_zone_name, ts.end_nyct2010_gid, ts.end_boroname, ts.end_ntacode, ts.end_ntaname, ts.end_taxi_zone_gid, ts.end_taxi_zone_name, weather.precipitation rain, weather.snow_depth, weather.snowfall, weather.max_temperature max_temp, weather.min_temperature min_temp, weather.average_wind_speed FROM trips_and_stations as ts LEFT JOIN central_park_weather_observations weather ON weather.date = ts.start_time::date ) TO PROGRAM 'split -l 10000000 --filter=\u0026quot;gzip \u0026gt; /mnt/c/Users/yourcompname/Documents/nyc-citibike-data/trips/trips_\\$FILE.csv.gz\u0026quot;' WITH CSV;  This will run in approximatel 15-30 mintues depending on your computer resources.\nLoading the Data into Clickhouse Once you have the files in place, you\u0026rsquo;ll want to start up clickhouse. If you need to review how to set it up you can check out my previous post. Once you\u0026rsquo;re all set, start your server and client:\nsudo service clickhouse-server start clickhouse client  Once in the client create your bike_trips table:\nCREATE TABLE bike_trips ( trip_duration Nullable(Float64), start_time Nullable(DateTime), stop_time Nullable(DateTime), start_station_id Nullable(String), start_station_name Nullable(String), start_station_latitude Nullable(Float64), start_station_longitude Nullable(Float64), end_station_id Nullable(String), end_station_name Nullable(String), end_station_latitude Nullable(Float64), end_station_longitude Nullable(Float64), bike_id Nullable(UInt8), user_type Nullable(String), birth_year Nullable(UInt8), gender Nullable(String), start_nyct2010_gid Nullable(Int8), start_boroname Nullable(String), start_ntacode Nullable(String), start_ntaname Nullable(String), start_taxi_zone_gid Nullable(Int8), start_taxi_zone_name Nullable(String), end_nyct2010_gid Nullable(Int8), end_boroname Nullable(String), end_ntacode Nullable(String), end_ntaname Nullable(String), end_taxi_zone_gid Nullable(Int8), end_taxi_zone_name Nullable(String), rain Nullable(Float64), snow_depth Nullable(Float64), snowfall Nullable(Float64), max_temp Nullable(Float64), min_temp Nullable(Float64), average_wind_speed Nullable(Float64) ) ENGINE = Log;  You may wonder why some fields that could be integers or float64 are changed to string. I decided to do this to get the data in quick, opting to optimize and tune later. In order to clean out some of the NULL data I borrowed Mark\u0026rsquo;s cleaning script he employs with the taxi data in clickhouse. You\u0026rsquo;ll want to log out of clickhouse client and back in terminal run the following:\ncd /mnt/c/Users/yourcompname/Documents/nyc-citibike-data/trips/ time (for filename in /mnt/c/Users/0000/Documents/nyc-citibike-data/trips/trips_x*.csv.gz; do gunzip -c $filename | \\ python trans.py | \\ clickhouse-client \\ --query=\u0026quot;INSERT INTO bike_trips FORMAT CSV\u0026quot; done)  Data should load rather quickly, the whole process took me about an hour. At the moment the script kicks and error at the very end, some dirty data that needs to be cleaned but I\u0026rsquo;ll get to that another time. Either way, log back into clickhouse client and run a few quick codes:\nSELECT COUNT(1) FROM bike_trips;  SELECT user_type, count(*) FROM bike_trips GROUP BY user_type;  And that\u0026rsquo;s it, quick simple and incredibly powerful, lightning fast results perfect for analytics. Enjoy!\n","date":1547960400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547960400,"objectID":"0a96de5c4eca5cbf4db8d96e6637fb5a","permalink":"/post/clickhouse_citibike/","publishdate":"2019-01-20T00:00:00-05:00","relpermalink":"/post/clickhouse_citibike/","section":"post","summary":"A quick tutorial on loading citibike data into a clickhouse database.","tags":["Academic"],"title":"Loading 50 Million Citibike Trips into Clickhouse on WSL","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":"What is Tabix? As part of my clickhouse setup series, I wanted to have a gui to work with, especally as a way for analysts without R and Python proficiency to query the database. Tabix seemed perfect for this; in addition the software had dashboarding capabilities for automated reports which seemed perfect.\n##Connecting\nSetting up tabix turns out to be incredibly simple. If you have followed my previous posts and set up clickhouse under wsl, then clickhouse will be running off localhost. First make sure cickhouse is running in wsl:\nSince tabix can run off your browser it doesn\u0026rsquo;t even need to be installed. Simply launch the (browser)[http://ui.tabix.io/] and you will be prompted to connect:\nUnder name simply set up a name for your connection, in my case I called it local_ch, then point tabix to local host port 8123. Note that if you are setting up remotely this will be different. I plan to do a tutorial on remote setup later. Under login you\u0026rsquo;ll want to type \u0026lsquo;default\u0026rsquo; since in my set up I haven\u0026rsquo;t added security; once again this will change if we set up remote systems and security in clickhouse. Once you are set, click sign in (I chose to click add new as well in order to store the data).\n#Querying\nOnce you sign in you will see the querying screen which is pretty intutive. Try and run the first query we ran from Mark Litwintschik\u0026rsquo;s blog. You should receive the following error:\nTo my knowledge this occurs becuse query time is greater than 5 seconds which is the default query time. Note that query time will vary based on your specs and tuning. In my case I didn\u0026rsquo;t tune the databse yet, and I also didn\u0026rsquo;t create the mergetree version of the table Mark did yet (you will see he creates a smaller table to query with optimized fields in his blog). Regardless I wanted to pump up the query time; all you have to do is go to settings on the top right and change the max exexution time in the new window from 5 to what your want (I put in 100 seconds):\nClick apply on the bottom and you should be good to go. Rerun the query and voila:\nDepending on the data you loaded you\u0026rsquo;ll have different numbers, note you can use the x circled in red to save your table.\n##Conclusions And that\u0026rsquo;s how easy it is to connect tabix to your clickhouse instance locally under wsl. In my next post in this series I\u0026rsquo;ll explore how to set up some quick graphs for dashboard reports, something that is very valuable when dealing with large data like this.\n","date":1545454800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545454800,"objectID":"b9228f09a3333a0004f4510b281f7558","permalink":"/post/using_tabix_windows/","publishdate":"2018-12-22T00:00:00-05:00","relpermalink":"/post/using_tabix_windows/","section":"post","summary":"A quick guide connecting your local clickhouse database to Tabix for business intelligence.","tags":["Academic"],"title":"Using Tabix in Windows WSL","type":"post"},{"authors":null,"categories":[],"content":" Why use Python or R? I think a lot of people wonder why we would use R or python when there are behemoths like Spark and Clickhouse able to crunch data quickly, but the reality is that these database languages weren’t necesarrily set up to do a lot of the more specific data munging work that R and python can do. A lot of data sceintists know that database languages and scripting lanuageges compliment each other.\nIn some of my previous posts I set up a clickhouse database under the wsl system in windows, a bit of a tricky setup but for the purpose of testing locally as well as a potential prototype for the company. In thise post I’m going to focus on accessing the data via R and Python.\n R Access The leading package for clickhouse access in R at the moment is RClickHouse, which comes with dplyr integration for those who enjoy piping. I’m more of a datatable guy, but I appreciate flexibiity. You’ll want to install the package and connect to the database; if you followed my previous post, it will be at the localhost address:\n#install.packages(\u0026#39;RClickhouse\u0026#39;) library(RClickhouse) library(DBI) library(dplyr) con = dbConnect(RClickhouse::clickhouse() , host=\u0026quot;localhost\u0026quot;) Once you’re connected you can begin querying. We will start off with some basic sql:\ndbGetQuery(con, \u0026quot;SELECT cab_type, count(*) FROM trips GROUP BY cab_type\u0026quot;) ## Warning in evalq((function (..., call. = TRUE, immediate. = FALSE, ## noBreaks. = FALSE, : column count() converted from UInt64 to Numeric ## cab_type count() ## 1 yellow 759083667 ## 2 green 69145084 If you’re a fan of dplyr you can pull that way as well:\ntrips = tbl(con, \u0026quot;trips\u0026quot;) %\u0026gt;% group_by(cab_type) %\u0026gt;% summarise(trips=sum(tip_amount, na.rm = T)) trips ## # Source: lazy query [?? x 2] ## # Database: clickhouse 19.1.6 [default@localhost:9000/default; uptime: 0 ## # days ] ## cab_type trips ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 yellow 1242908568. ## 2 green 80399851. And when you’re done with the code you can disconnect from the database:\n# Close the connection dbDisconnect(con)  Python Access Python access is very straightforward. You’ll want to reference the documentation. Simply install the driver, I use anaconda for most of my python operations so easily pulled up the terminal and ran the pip install. Next you’ll want to run the following code to enter clickhouse through python and see existing tables:\nfrom clickhouse_driver import Client client = Client(\u0026#39;localhost\u0026#39;) client.execute(\u0026#39;SHOW TABLES\u0026#39;) client.execute(\u0026#39;SELECT sum(rain) FROM trips\u0026#39;) In my case I have mutiple tables; you can also see I went ahead and executed a sql query:\n  Conclusions This is a great way to access large data with cutting edge database technology throough your favorite scripting language on a windows machines still running the open source software you love. I see it as a good opportunity for teams working within a windows enviornment but needing access to linux software. In one of my next posts I’ll build a quick shiny gui to access the data through a custom front end.\n ","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"3c4c936e61217bc607e111bbb085dbdc","permalink":"/post/accessing-clickhouse-data-in-r-python/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/post/accessing-clickhouse-data-in-r-python/","section":"post","summary":"Why use Python or R? I think a lot of people wonder why we would use R or python when there are behemoths like Spark and Clickhouse able to crunch data quickly, but the reality is that these database languages weren’t necesarrily set up to do a lot of the more specific data munging work that R and python can do.","tags":["database","r","python"],"title":"Accessing Clickhouse Data in R \u0026 Python","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":"Why ClickHouse? Recently I\u0026rsquo;ve been wanting to expand my knowledge of other database types beyond SQL variants and map reduce methods with spark. As a data sceintist at the Taxi \u0026amp; Limousine Commission I follow some of the work that is done by the public using our data and have followed Mark Litwintschik\u0026rsquo;s blog blog for some time. At work we currently lean on SQL Server and PostgreSQL along with Python and R for most of our analyses but I\u0026rsquo;ve been curious about incentivizing change (albeit a resistant IT department as many of you probably have).\nIn 2017 Mark benchmarked 1.1 billion taxi rides using Todd Schneider\u0026rsquo;s ETL scripts with our public data. It\u0026rsquo;s been some time but I thought I\u0026rsquo;d give it a shot myself, particularly since I was intrigued by the performance gains over postgres and even spark on a single node machine.\nI didn\u0026rsquo;t want to spin up a cluster on Amazon to pay at the moment, ot to mention all the storage I would need to buy, so I decided to run this on my Dell G7 laptop which has some impresive specs for a cheap laptop:\nSince clickhouse only runs on linux distributions I chose to run it on Ubuntu 18.04 on WSL (Windows Subsystem Linux). The reason I didn\u0026rsquo;t choose virtualbox was because I wanted to leverage the full power on my computer, and later test Windows HouseOps functionality to access the server locally, a nifty setup to convince the higher ups to use this.\n##Getting WSL up and Running\nInstalling wsl in windows is simple, with documentation here. Simply activate via powershell and then install through the windows store (choice 1 from the 3 choices listed in the documentation).\nOnce Ubuntu is installed, you\u0026rsquo;ll want to open up ubuntu through cortana and begin standard linux setup, adding in your password and user information. You\u0026rsquo;ll want to run the usual update and upgrade values:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade\nSince I was just testing out the system and wsl is behind windows firewall I didn\u0026rsquo;t setup any firewalls or security.\n##PostgreSQL\nI followed Mark\u0026rsquo;s blog on coding the data through postgres in order to produce the gz files we will load into clickhouse. You can follow the instructions there but here is a short summary of what to do. You\u0026rsquo;ll want to install postgres client and server. In ubuntu, make a directory for the taxi data and clone Todd\u0026rsquo;s repo (note I chose to place my files in windows in order to play around with accessing files across platforms; this means you will need to access the windows files directories):\nsudo apt-get install postgresql mkdir /mnt/c/Users/0000/Desktop/Documents cd /mnt/c/Users/0000/Desktop/Documents/nyc-taxi-data git clone https://github.com/toddwschneider/nyc-taxi-data.git cd /mnt/c/Users/0000/Desktop/Documents/nyc-taxi-data\nyou\u0026rsquo;ll download the data automatically, but remember that Todd\u0026rsquo;s script pulls the urls of ALL the trip records. If you just want to test first with only a year or so of data I reccomend opening the url script with something like:\nsudo nano /mnt/c/Users/0000/Desktop/Documents/nyc-taxi-data/setup_files/raw_data_urls.txt\nand deleting as many files as you wish, so you can run:\n./download_raw_data.sh \u0026amp;\u0026amp; ./remove_bad_rows.sh\nOnce you\u0026rsquo;ve downloaded the data, you can run the initialize and import scripts:\n./import_trip_data.sh ./import_fhv_trip_data.sh\nI chose to do this in chunks as I did not have the hard drive space in place. This is definately the longest part of the process. Once it\u0026rsquo;s done, you will now have a postgresql database with all the data loaded in which you can query. This means we can pull the data out and compress it into .gz files which will be loaded into clickhouse. You\u0026rsquo;ll want to:\n create the directory: mkdir -p /mnt/c/Users/0000/Desktop/Documents/nyc-taxi-data/trips add permissions to write to the directory: sudo chown -R postgres:postgres \\ /mnt/c/Users/0000/Desktop/Documents/nyc-taxi-data/trips enter postgres: psql nyc-taxi-data  And then run the script to pull the data:\nCOPY ( SELECT trips.id, trips.vendor_id, trips.pickup_datetime, trips.dropoff_datetime, trips.store_and_fwd_flag, trips.rate_code_id, trips.pickup_longitude, trips.pickup_latitude, trips.dropoff_longitude, trips.dropoff_latitude, trips.passenger_count, trips.trip_distance, trips.fare_amount, trips.extra, trips.mta_tax, trips.tip_amount, trips.tolls_amount, trips.ehail_fee, trips.improvement_surcharge, trips.total_amount, trips.payment_type, trips.trip_type, trips.pickup_location_id, trips.dropoff_location_id, cab_types.type cab_type, weather.precipitation rain, weather.snow_depth, weather.snowfall, weather.max_temperature max_temp, weather.min_temperature min_temp, weather.average_wind_speed wind, pick_up.gid pickup_nyct2010_gid, pick_up.ctlabel pickup_ctlabel, pick_up.borocode pickup_borocode, pick_up.boroname pickup_boroname, pick_up.ct2010 pickup_ct2010, pick_up.boroct2010 pickup_boroct2010, pick_up.cdeligibil pickup_cdeligibil, pick_up.ntacode pickup_ntacode, pick_up.ntaname pickup_ntaname, pick_up.puma pickup_puma, drop_off.gid dropoff_nyct2010_gid, drop_off.ctlabel dropoff_ctlabel, drop_off.borocode dropoff_borocode, drop_off.boroname dropoff_boroname, drop_off.ct2010 dropoff_ct2010, drop_off.boroct2010 dropoff_boroct2010, drop_off.cdeligibil dropoff_cdeligibil, drop_off.ntacode dropoff_ntacode, drop_off.ntaname dropoff_ntaname, drop_off.puma dropoff_puma FROM trips LEFT JOIN cab_types ON trips.cab_type_id = cab_types.id LEFT JOIN central_park_weather_observations weather ON weather.date = trips.pickup_datetime::date LEFT JOIN nyct2010 pick_up ON pick_up.gid = trips.pickup_nyct2010_gid LEFT JOIN nyct2010 drop_off ON drop_off.gid = trips.dropoff_nyct2010_gid ) TO PROGRAM 'split -l 20000000 --filter=\u0026quot;gzip \u0026gt; /mnt/c/Users/0000/Desktop/db/latest/nyc-taxi-data/trips/trips_\\$FILE.csv.gz\u0026quot;' WITH CSV;  Once you finish downloading the files you are read to load them into clickhouse. The beauty of these files is that you can use them again and again to explore some of the other databases that Mark benchmarks or otherwise explore options you may want to try.\n##Setting up clickhouse in wsl\nThis was certainly the trickiest part of this porcess. Clickhouse documentation asks for the following code segments:\ndeb http://repo.yandex.ru/clickhouse/deb/stable/ main/ sudo apt-get install dirmngr # optional sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E0C56BD4 # optional sudo apt-get update sudo apt-get install clickhouse-client clickhouse-server  If you run these in wsl you will receive an error that says dbgmanager is not installed. WSL to my knowledge has not solved CURL issues through dbmanager so changing keys from sources proves difficult. There are a few options noted in serveral forums that you can see here, but in my case I decided to install clickhouse manually. Simply go to the repository and install matching versions, of the commons, server and client:\nclickhouse-common-static_19.1.6_amd64.deb clickhouse-server_19.1.6_all.deb clickhouse-client_19.1.6_all.deb  In my case they hit the downloads folder which you can reference and install:\ncd /mnt/c/Users/0000/Desktop/Downloads sudo dpkg -i /path/to/deb/file\nNote you will do this for al three files. Follow the prompts and you should be able to install the software without a hitch. Once everything is installed, start the service:\nsudo service clickhouse-server start\nStart the client:\nclickhouse client\nYou are now in the equivalent of the psql database in clickhouse.In here you can run all your database queries, you can create a database and begin making tables. You will want to create the trips table in order to populate it with the data from the .gz files. You can run the following code:\nCREATE TABLE trips ( id UInt32, vendor_id String, pickup_datetime DateTime, dropoff_datetime Nullable(DateTime), store_and_fwd_flag Nullable(FixedString(1)), rate_code_id Nullable(UInt8), pickup_longitude Nullable(Float64), pickup_latitude Nullable(Float64), dropoff_longitude Nullable(Float64), dropoff_latitude Nullable(Float64), passenger_count Nullable(UInt8), trip_distance Nullable(Float64), fare_amount Nullable(Float32), extra Nullable(Float32), mta_tax Nullable(Float32), tip_amount Nullable(Float32), tolls_amount Nullable(Float32), ehail_fee Nullable(Float32), improvement_surcharge Nullable(Float32), total_amount Nullable(Float32), payment_type Nullable(String), trip_type Nullable(UInt8), pickup_location_id Nullable(UInt8), dropoff_location_id Nullable(UInt8), cab_type Nullable(String), rain\tNullable(Float32), snow_depth Nullable(Float32), snowfall Nullable(Float32), max_temperature Nullable(Float32), min_temperature Nullable(Float32), wind\tNullable(Float32), pickup_nyct2010_gid Nullable(Int8), pickup_ctlabel Nullable(String), pickup_borocode Nullable(Int8), pickup_boroname Nullable(String), pickup_ct2010 Nullable(String), pickup_boroct2010 Nullable(String), pickup_cdeligibil Nullable(FixedString(1)), pickup_ntacode Nullable(String), pickup_ntaname Nullable(String), pickup_puma Nullable(String), dropoff_nyct2010_gid Nullable(UInt8), dropoff_ctlabel Nullable(String), dropoff_borocode Nullable(UInt8), dropoff_boroname Nullable(String), dropoff_ct2010 Nullable(String), dropoff_boroct2010 Nullable(String), dropoff_cdeligibil Nullable(String), dropoff_ntacode Nullable(String), dropoff_ntaname Nullable(String), dropoff_puma Nullable(String) ) ENGINE = Log;  and you should receive the following message:\nYou should be all set now to load the trips.\n##Loading the data into clickhouse\nLoading the data into clickhouse requires editing some of the fields in the trip data. Mark writes:\n-The dataset itself uses commas for delimiting fields. None of the contents of the data contains any commas themselves so there is no quotations used to aid escaping data. NULL values are defined by the simple absence of any content between the comma delimiters. Normally this isn\u0026rsquo;t an issue but with ClickHouse empty fields won\u0026rsquo;t be treated as NULLs in order to avoid ambiguity with empty strings. For this reason I need to pipe the data through a transformation script that will replace all empty values with \\N.\nSo in order to fix this we run his script. You will want to make sure that you install python and create the script:\nsudo apt-get install python sudo nano trans.py\nand then paste in the script text:\nimport sys for line in sys.stdin: print ','.join([item if len(item.strip()) else '\\N' for item in line.strip().split(',')])  Now you can run the upload script:\ntime (for filename in /mnt/c/Users/0000/Desktop/Documents/nyc-taxi-data/trips/trips_x*.csv.gz; do gunzip -c $filename | \\ python trans.py | \\ clickhouse-client \\ --query=\u0026quot;INSERT INTO trips FORMAT CSV\u0026quot; done)  You should now see clickhouse loading and processing data. This will vary on the amount of data but shouldn\u0026rsquo;t take more than a few hours tops.\n##Query and Test You can take this a step further, creating a mergetree table and tuning up. But you will want to run a few tests first. You can go head and access the data with some basic queries:\n----cab type SELECT cab_type, count(*) FROM trips GROUP BY cab_type ----puloc double group by select pickup_location_id, cab_type, count(pickup_datetime) FROM trips group by pickup_location_id, cab_type  You should see these queries run in a few seconds depending on how much data you loaded.\n##Conclusions and Next Steps Ultimately this is a very powerful software, particularly for low-resource departments with big data usage. This setup is certainly not the most optimal, but for quick access to large data for the average person, clickhouse seems to be a very powerful tool with great potential. A better setup would leverage pure linux distributions, but this can be a great option for organizations which for security reasons may not be able to run linux outside of a windows environment.In the next step I will try and run Tabix, which is a third party gui meant to allow for access of clickhouse data as well as test R and Python access to clickhouse via wsl. Stay tuned!\n","date":1544158800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544158800,"objectID":"d2302a0a981ba0c324b3c1427aed85bf","permalink":"/post/clickhouse_billion_rides_wsl/","publishdate":"2018-12-07T00:00:00-05:00","relpermalink":"/post/clickhouse_billion_rides_wsl/","section":"post","summary":"Loading a billion taxi rides into clickhouse on WSL as a proof of concept for low resource optimization.","tags":["Academic"],"title":"A Billion Taxi Rides in Clickhouse WSL","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":"Taxi Industry \u0026amp; Indicators\nThe Taxi \u0026amp; Limousine Commission of New York City (TLC) is the regulating body for all taxi and for hire vehicle service. In order to understand the industry they regulate data collection is key and quick metrics are important for tracking and getting ahead of shifts in an industry. TLC releases industry indicators.\nIn order to make this easier for the public, I created the tlc fast dash, a simple shiny application meant to help visualize these metrics for rapid use. The source code is available through the app or you can click right here. You can also see the official post at the tlc medium blog\nThis dashboard has gone through many iterations in the beginning of my shiny R career, you can see some of the iterations in snapshots from before:\nSoon to come is a more comprehensive dashboard, my team is working. Stay tuned.\n","date":1541394000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541394000,"objectID":"80f6a883b24b757d0e502f1b98400db6","permalink":"/post/taxi_industry_metrics/","publishdate":"2018-11-05T00:00:00-05:00","relpermalink":"/post/taxi_industry_metrics/","section":"post","summary":"Simple metrics for a complicated story.","tags":["Academic"],"title":"Taxi Analytics and KPIs through R Shiny","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":"Spark on AWS I haven\u0026rsquo;t had much exposure on AWS, considering that most of my work is on local databases like a SQL Server or PostgreSQL, but recently I have been exploring new database options. Spark has been on my list and I thought this a good moment to refresh on AWS and start familiarizing myself with more of what Amazon has to offer. I found a great tutorial by Zachariah W. Miller in which he covers spinning up a cluster and utilizing over 120 million flight records to test the effectiveness of spark as an anlaytics tool. Zack doesn\u0026rsquo;t cover the costs but for those following my process it cost me about 2 bucks to play on this cluster for an hour. Not too shabby!\nInitiating the Cluster You\u0026rsquo;ll want to make an account on AWS and then search for the EMR service in your console. Once you enter hit create a cluster and you should see the following: You\u0026rsquo;ll want to name your cluster and set up 3 nodes; also pick the last option that installs spark 2.4.0 in your configuration. Your cluster will comprise of a master node and the worker nodes under it. In addition to choosing the amount of nodes, you will also want to pick a key pair. A key pair allows you access to your instance through SSH so that you can go on your computer and enter the AWS master node. If you are just starting off then you will have to create a new one; as is often the case with microsoft windows in programming and data, things are more difficult than MacOS or Linux. First download putty, then you\u0026rsquo;ll want to create a key pair and convert that key pair with puttygen. Once you select your key pair click create cluster. If you\u0026rsquo;re a raspberrypi fan like myself this should be pretty familiar.\nThe cluster will take 5-10 minutes to finish initializing at which moment you will see waiting . . . Once you see this you can attempt to SSH into your master node. Note that when I tried this there was an issue with the master node; if you get a timeout error you need to go to your security group settings and add a rule for allowing any connection. Once you have this set up you can SSH in through puttygen, simply follow the instructions provided on SSH section next to your master public ip: And if all goes well you should get the following: #Loading the Data\nNow we will load the flight data; Amazon provides this data for testing it seems and we can grab it from S3 buckets that hold the raw csv files. This data has over 120 million records making it a good start to testing your big data chops (maybe more like medium data, even taxi data isn\u0026rsquo;t really that big). Since your are using AWS EMR to run this cluster the software has already installed Hadoop for you so loading data in HDFS format is easy. You can create a directory and load the data in:\nhadoop fs -mkdir /data #create a directory to mount the data hadoop fs -cp s3://dask-data/airline-data/*.csv /data/. #copy all csv files into the directory you created hadoop fs -ls /data #check your directory for the files (note you wil have to use hadoop command not just ls)  You should see something like this in your directory (note it took about 5-10 minutes to copy): Now that the data is copied we can start loading it. From what I\u0026rsquo;ve seen the easiest way to get up and running is with pyspark so you can go ahead and call pyspark in terminal and you\u0026rsquo;ll be greeted with the spark command line: You\u0026rsquo;ll notice that I\u0026rsquo;ve already typed in some commands. Spark uses sc which stands for SqlContext to tie python to spark; we will want to use this to get SparkSQL setup. Run sqlContext = SQLContext(sc) to create your connection. Now we can attempt to load the data from HDFS and use Spark\u0026rsquo;s automatic schema detection to attempt to guess our table\u0026rsquo;s data types. You do this by running:\ndf = sqlContext.read.format('com.databricks.spark.csv')\\ .options(header='true', inferschema='true').load('hdfs:///data/*.csv')  This will take a bit of time as the data is loaded in csv format from all over the cluster. Once the progress bar completes you can check your dataframe to see what the schema shows:\ndf.printSchema()  Note that some of the columns that should be numeric were loaded as strings; the failsafe is often to load data in as a string format since it has few restrictions. We will need to convert these values later when we want to attempt a serious analysis but for now we can solve this quickly by casting values to floats.\nAnalyzing the Data Set with Pandas We can start to analyze the data with some quick functions imported from pandas:\nfrom pyspark.sql.types import FloatType df.withColumn(\u0026quot;DepDelay\u0026quot;, df[\u0026quot;DepDelay\u0026quot;].cast(FloatType()))\\ .select(['Year','DepDelay']).groupby('Year').mean().orderBy('Year').show(50)  Here we cast DepDelay as a float in order to apply mathematical operations and group by the year:\nWe can run a similar operation on arrivals:\ndf.withColumn(\u0026quot;ArrDelay\u0026quot;, df[\u0026quot;ArrDelay\u0026quot;].cast(FloatType()))\\ .select(['Year','ArrDelay']).groupby('Year').mean().orderBy('Year').show(50)  We can group by more than just one variable very easily:\ndf.withColumn(\u0026quot;ArrDelay\u0026quot;, df[\u0026quot;ArrDelay\u0026quot;].cast(FloatType()))\\ .select(['Year','Month','ArrDelay']).groupby('Year','Month').mean().orderBy('Year').show(50)  And what about Scala? Well I don\u0026rsquo;t know much about the language yet but I did want to at least test out a bit of it before I terminated the cluster (And you will want to terminate the cluster!). I found that you could access scala shell by typing in the following:\nspark-shell  I went ahead and made my first scala list:\nI repeat: make sure you terminate your cluster!\nMachine Learning Next? Using spark on AWS was quick and relatively simple, in fact the hardest part was navigating the giant AWS ecosystem, something that required a few days of studying to fully understand, but now I am starting to see all the possibilities. Zachariah includes a few parts to this tutorial one of which covers machine learning which I am eager to follow next. I think I will blow through this series and then attempt to apply a similar process to the taxi data I work with at TLC.\n","date":1533096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533096000,"objectID":"562e24968ae1ddaaf6c7de0b85e44cac","permalink":"/post/aws_1/","publishdate":"2018-08-01T00:00:00-04:00","relpermalink":"/post/aws_1/","section":"post","summary":"Spinning up a spark cluster in AWS.","tags":["Academic"],"title":"Creating a Spark Cluster on AWS with 120 Million Flight Records","type":"post"},{"authors":["James Parrot","Michael Reich","TLC Support"],"categories":null,"content":"Notes.\n","date":1530417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530417600,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2018-07-01T00:00:00-04:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We provide the first detailed analysis of the app-based transportation industry in a large metropolis. Concerned about reports of low earnings (after costs) among drivers working for the large app-based for-hire vehicle (FHV) companies the New York City Taxi and Limousine Commission (TLC) wishes to establish a minimum driver pay standard. The policy would set an earnings floor of $17.22, the independent contractor equivalent of a $15 hourly wage, with an allowance for paid time off. We examine the need for and likely effects of the TLC’s proposed policy. Our analysis draws mainly upon administrative data collected from all the companies by the TLC, and we develop a model to simulate the effects of the policy. We find that a majority of the city’s FHV drivers work full-time and that 85 percent make less than the proposed pay standard. Hourly pay is low in large part because the industry depends upon a ready availability of idle drivers to minimize passenger wait times. The proposed policy would increase driver net earnings (after expenses) by 22.5 percent, or an average of $6,345 per year among the 85 percent of drivers who would get increases. At the same time, company commissions in the city generate very large mark-ups over local operating costs The policy could be fully paid for by combining an increase of 2.4 minutes in driver trips with passengers per working hour with reductions in company commissions. Fare increases would then be small (five percent or less) and average wait times for passengers would increase by about 12 to 15 seconds. The policy would reward drivers for pooled rides, which would increase as a share of all rides. The policy would substantially reduce growth in the number of new drivers and vehicles and provide some indirect benefits for medallion drivers.","tags":[],"title":"An Earnings Standard for New York City’s App-based Drivers: Economic Analysis and Policy Assessment","type":"publication"},{"authors":["Fausto Lopez"],"categories":null,"content":"Why Shiny? When I started working at TLC I wanted to use R\u0026rsquo;s Shiny package to help develop some quick and easy to use dashboards that would streamline my work flow and automate a lot of data analytics for the team. At the the time I built a few for internal use, but getting them to run across the agency was difficult; the only package known for local install was RInno and admin priveleges for this sort of mucking around was frowned upon. The powers at be were not inclined to utilize shiny as they saw it as an open source security issue and so we resolved to use Tableau. I still released a dashboard for public use using shiny.io but that was limited to public data.\nRecently the team has been running into limitations with tableau and myself and some of my creative staff are working with shiny apps as they offer more flexibility and are just more fun to make. As a result I\u0026rsquo;ve been toying with the idea of setting up our own shiny server infrastructure, and since it\u0026rsquo;s also been something on my own personal to-do list, I figured why not (I had done this with a raspberry pi before albeit for fun). Whatever I learn I can use for my own app development and later scale it to the workplace. But wait, why shiny and not flask or django? Well aside from the existing work we are already doing, I think shiny makes for easier development with decent room for scaling, it\u0026rsquo;s free and has a vibrant community as with all things R. As useful as python is for some of my other work, R\u0026rsquo;s just easier in my opinion, just think about the last time you needed to install a package in R vs. Python at work!?\nI went through this process on EC2 but there are a lot of tutorials on how to do this on digital ocean as well as on EC2. I used the following sources in combination for my install process:\n The great Dean Attali installs shiny server on digital ocean\n The clever Catherine Ordun installs shiny server on aws EC2\nSetup on AWS Another thing on the old to-do list was using AWS which I haven\u0026rsquo;t used much, always defaulting to the rebel favorite digital ocean, and I thought I could go ahead and kill two birds with one stone so I decided to setup the shiny server on AWS ec2, something which ended up being a great decision as I\u0026rsquo;ve gotten much better with AWS since this process.\nFirst you\u0026rsquo;ll want to navigate to free tier options and choose the 750 hours of EC2 per month: Next you\u0026rsquo;ll want to choose your instance (make sure it says free, I chose 16.04 Ubuntu): You\u0026rsquo;ll be prompted to set up a key pair, if you\u0026rsquo;ve used AWS before you can go ahead and choose the key pair you already made or else you\u0026rsquo;ll have to make your own. Once you move through the rest of the defaults you can launch your instance: And you wait until you see it running: Connect to the your Instance Connecting to your instance can be a headache if you haven\u0026rsquo;t done it before. If you are just starting off then you will have to create a new one; as is often the case with Microsoft windows in programming and data, things are more difficult than MacOS or Linux. First download putty, then you’ll want to create a key pair and convert that key pair with puttygen. Now you can feed this converted pair into your puttygen app to connect. Note that when you finally get that terminal open, your login is ubuntu with no password.\nInstall R I like the idea of having a mobile rstudio option, so as Dean Attali does in his install procedure I went ahead and set that up as well. First I updated and added nginx:\nsudo apt-get update sudo apt-get -y install nginx  To test that this worked you can visit your ip address which will look something like http://ec2- . . . .com and you should see a welcoming from nginx. Dean points out when he tests this in digital ocean:\n\u0026ldquo;The default file that is served is located at /var/www/html/index.nginx-debian.html, so if you want to change what that webpage is showing, just edit that file with sudo nano /var/www/html/index.nginx-debian.html. For example, I just put a bit of text redirecting to other pages in my index page. The configuration file is located at /etc/nginx/nginx.conf.\nWhen you edit an HTML file, you will be able to see the changes immediately when you refresh the page, but if you make configuration changes, you need to restart nginx.\u0026rdquo;\nYou can always manipulate nginx with:\nsudo service nginx stop sudo service nginx start sudo service nginx restart  Next we can add R to the source list:\nsudo sh -c 'echo \u0026quot;deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\u0026quot; \u0026gt;\u0026gt; /etc/apt/sources.list  The above will install R 3.4, if you want 3.5 and above run:\nsudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/'  And add public keys:\ngpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9 gpg -a --export E084DAB9 | sudo apt-key add -  And install R:\nsudo apt-get update sudo apt-get -y install r-base  Go ahead and test R by running R and you should get the pop up window: On EC2 I chose the free tier, which is the weakest machine with 1GB of ram. Dean points out that you can add 1GB of swap space to allow larger packages to be installed:\nsudo /bin/dd if=/dev/zero of=/var/swap.1 bs=1M count=1024 sudo /sbin/mkswap /var/swap.1 sudo /sbin/swapon /var/swap.1 sudo sh -c 'echo \u0026quot;/var/swap.1 swap swap defaults 0 0 \u0026quot; \u0026gt;\u0026gt; /etc/fstab'  Once this is completed we install dependencies for R\u0026rsquo;s devtools along with devtools and Shiny:\nsudo apt-get -y install libcurl4-gnutls-dev libxml2-dev libssl-dev sudo su - -c \u0026quot;R -e \\\u0026quot;install.packages('devtools', repos='http://cran.rstudio.com/')\\\u0026quot;\u0026quot; sudo su - -c \u0026quot;R -e \\\u0026quot;devtools::install_github('daattali/shinyjs')\\\u0026quot;\u0026quot;  Important: R Package Install! In order to run certain packages across all users we can\u0026rsquo;t just install in the local R instance you pull up. We will want to leverage root priveleges to install the packages globally. That\u0026rsquo;s why above we run sudo su - -c in order to get that access.\nInstalling Rstudio Server Now that we have R installed we can install Rstudio server which you will be able to access from anywhere with a computer; start with the dependencies:\nsudo apt-get -y install gdebi-core  Next grab the latest version of rstudio; note that you will have to swap the download link for the latest one on the downloads page:\nwget https://download2.rstudio.org/rstudio-server-1.1.463-amd64.deb sudo gdebi rstudio-server-1.1.463-amd64.deb  Before we access rstudio server we have to make sure that our EC2 security settings allow for connections. This is not the most secure way to do things, but at this moment I am just proving a concept. You\u0026rsquo;ll want to navigate to your EC2 dashboard and then down to security groups. Once there locate the security group associated with your instance and edit the inbound rules:\nThis will open up a window where you can edit the port connections:\nHere we are allowing a connection from anywhere to hit the relevant ports. You will add rules for port 8787, 3838 and 80 because we will use all these to access our server.\nAnd now you should be able to access rstudio server in your browser with your ec2 address: When I got here initially I was not sure how to login, as I had not created another user; the process in EC2 looked a little trickier and I was running into errors. My workaround was to create a password for the ubuntu un which is the default. Simply run:\nsudo passwd ubuntu  Enter a password and you should be good to go.\nInstalling Shiny Server We finally reach the main portion of this tutorial which is installing shiny server! You\u0026rsquo;ll start by importing the shiny package needed to run shiny server an we will bring in the rmarkdown package as well since shiny server lets you run rmarkdown documents:\nsudo su - -c \u0026quot;R -e \\\u0026quot;install.packages('shiny', repos='http://cran.rstudio.com/')\\\u0026quot;\u0026quot; sudo su - -c \u0026quot;R -e \\\u0026quot;install.packages('rmarkdown', repos='http://cran.rstudio.com/')\\\u0026quot;\u0026quot;  Just like before you\u0026rsquo;ll want to grab the latest version of shiny server:\nwget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.9.923-amd64.deb sudo gdebi shiny-server-1.5.9.923-amd64.deb  Shiny will let you know it\u0026rsquo;s running:\nOnce this runs you will be able to hit your EC2 address again as with rstudio server but instead of 8787 as your port you will point the address to 3838 and you will get:\nAdd write and read priveleges sudo groupadd shiny-apps sudo usermod -aG shiny-apps ubuntu sudo usermod -aG shiny-apps shiny cd /srv/shiny-server sudo chown -R ubuntu:shiny-apps . sudo chmod g+w . sudo chmod g+s .  Deploy a Shiny App using Git More to come here as I update this post\nissues with rgdal run: sudo add-apt-repository -y ppa:ubuntugis/ubuntugis-unstable sudo apt update sudo apt install gdal-bin python-gdal python3-gdal libgdal1-dev sudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev install.packages(\u0026ldquo;rgdal\u0026rdquo;)\n","date":1530417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530417600,"objectID":"38835e3cbc7751ef5ec1836dee3ad66d","permalink":"/post/setting_up_shiny_server/","publishdate":"2018-07-01T00:00:00-04:00","relpermalink":"/post/setting_up_shiny_server/","section":"post","summary":"Spinning up a spark cluster in AWS.","tags":["Academic"],"title":"Setting up a Shiny Server on AWS EC2","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":"","date":1526529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526529600,"objectID":"fdf53be6090b49b1d503c39b531d4e2c","permalink":"/talk/example2/","publishdate":"2018-05-17T00:00:00-04:00","relpermalink":"/talk/example2/","section":"talk","summary":"A fun talk with Open Data staff at Google's Civi Hall offices, I had a chance to present some of the work we do at TLC, interesting trends and fun projects. You can watch the video of the presentation [here](https://www.facebook.com/CivicHallNYC/videos/2042845915785394/).","tags":null,"title":"Socrata Connect 2018: TLC \u0026 the Taxi Industry","type":"talk"},{"authors":null,"categories":[],"content":" Video Analytics I had a recent project proposal in which I was to do something very interesting with data and rather than download some data set to work with I thought it be fun to work on something new. For some time I have been interested in getting into computer vision and now that I build and fly drones merging the two just seems inevitable. So I spent the last weekend learning how to work with this, most of it in python and then a little bit of it in R for fun. The ultimate goal: to run object detection on drones.\nAnother note: a lot of this work as usual comes from learning from great people who help the community. Credits to Harrison at https://pythonprogramming.net/ who hosts an entire youtube tutorial series on opencv not to mention teaches everything for Python. I recommend starting off with this:\nhttps://www.youtube.com/watch?v=Z78zbnLlPUA\u0026amp;list=PLQVvvaa0QuDdttJXlLtAJxJetJcqmqlQq\n Computer Vision in R? What? Why? R is my bread and butter, and I love the language; for those who aren’t as familiar with python this could be a gentle transition into the computer vision world (albeit there is still so much to learn!) and so why not?\n What do I need? So before we jump into the code let’s go over the dependencies we will need because there are quite a few. A note of caution, setups differ and I am by no means an expert in installation procedures but I will give you my setup:\n****Running Windows 10 Surface\nPython\n-for python, you’ll have to decide which version to install, 2.7 o3 3.xx, some people will tell you to install Anaconda. Personally I have all three, but for windows video (one link is Harrison’s run of the install):\nhttps://www.python.org/downloads/release/python-352/ https://www.youtube.com/watch?v=ulJdZn0qBCQ\nOpenCV\n-if you follow the youtube video above it should be installed with python 3.5.2\nR\nR is a little easier, install the most recent R version (google it). Install devtools and rtools (just google it, very simple) and then install the ROpenCVlite and Rvision:\nhttps://github.com/swarm-lab/ROpenCVLite https://github.com/swarm-lab/Rvision\nROpenCVLite is just going to reinstall OpenCV in the right location for Rvision and future R computer vision libraries to access.\n Can we start writing some Code? Now on to the fun stuff. We can start with R; first lets pull the video file we need (drone video from a detection data set I’m working with):\n#directory setwd(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py\u0026quot;) #the name of the file my_path =paste0(getwd(),\u0026quot;/\u0026quot;,\u0026quot;Video_1.avi\u0026quot;) #download the video download.file(url = \u0026quot;https://drive.switch.ch/index.php/s/3b3bdbd6f8fb61e05d8b0560667ea992/download?path=%2Fvideos%2Fdrones\u0026amp;files=Video_1.avi\u0026quot; ,destfile = my_path ,mode = \u0026quot;wb\u0026quot; ) Once you’ve downloaded the video into a directory we can begin to parse through it; we can check the number of frames in the video and then extract one to plot:\nlibrary(Rvision) #rvision wants us to create a video object the_vid = Rvision::video(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py/Video_1.avi\u0026quot;) nframes(the_vid)#shows us the number of frames ## [1] 393 my_frame = readFrame(the_vid,3) plot(my_frame) release(the_vid) #i believe like closing an odbc connection this is good practice ## Video released successfully. Now that we have the video in place and know how to extract one frame we can simply loop through the frames and save them as images to train our models:\nlibrary(pbapply) #rvision wants us to create a video object the_vid = video(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py/Video_1.avi\u0026quot;) #then we loop through that video object and extract the frame pblapply(1:10, function(x){ z = readFrame(the_vid,x) setwd(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py\u0026quot;) write.Image(z,paste0(x,\u0026quot;_\u0026quot;,\u0026quot;frame\u0026quot;,\u0026quot;.jpg\u0026quot;)) }) ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL ## ## [[8]] ## NULL ## ## [[9]] ## NULL ## ## [[10]] ## NULL #for the newbies who perfer the for loops syntax: # for (i in 1:10) { # the_vid = video(\u0026quot;C:\\\\Users\\\\fausto\\\\Documents\\\\incubator_img_recog\\\\plane_classifier\\\\drone_proj\\\\videos\\\\drones\\\\Video_1.avi\u0026quot;) # z = readFrame(the_vid,i) # plot(z) # } setwd(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py\u0026quot;) list.files()  ## [1] \u0026quot;1_frame.jpg\u0026quot; \u0026quot;10_frame.jpg\u0026quot; \u0026quot;2_frame.jpg\u0026quot; ## [4] \u0026quot;3_frame.jpg\u0026quot; \u0026quot;4_frame.jpg\u0026quot; \u0026quot;5_frame.jpg\u0026quot; ## [7] \u0026quot;6_frame.jpg\u0026quot; \u0026quot;7_frame.jpg\u0026quot; \u0026quot;8_frame.jpg\u0026quot; ## [10] \u0026quot;9_frame.jpg\u0026quot; \u0026quot;vid_frames_r_py.Rmd\u0026quot; \u0026quot;Video_1.avi\u0026quot; And that’s about it for downloading video data in R. Insert grayscaling functions into the loop, or other analyses with the EBI or imageR package.\n How about Python? Since we already have the video downloaded in R I won’t bring it in again python. Instead I will run through how to pull the frames and save them. You can run the following:\nimport cv2 import numpy as np import os #set directory os.chdir(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py\u0026quot;) #pull in video cap = cv2.VideoCapture(\u0026quot;Video_1.avi\u0026quot;) count = 0 #success =T #loop through video and pull frames for saving while True: ret, img = cap.read() print \u0026#39;Read a new frame: \u0026#39;, ret gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) os.chdir(\u0026quot;C:/Users/0000/Documents/original_web_posts/video_prc_r_py\u0026quot;) cv2.imwrite(\u0026quot;frame%d.jpg\u0026quot; % count, img) # save frame as JPEG file count += 1 The code will kick and error at the end as it runs out of frames, you can adjust to get rid of that. And there it is! A few lines of code and you can already extract frames from videos. In one of my next posts I’ll go over how to use some of that picture data to train a basic Haarcascade so we can detect specific objects in videos.\n ","date":1519344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519344000,"objectID":"1900e9e21efa97336c0cadc9b3aeef67","permalink":"/post/downloading-extracting-video-frames-in-r-python/","publishdate":"2018-02-23T00:00:00Z","relpermalink":"/post/downloading-extracting-video-frames-in-r-python/","section":"post","summary":"Video Analytics I had a recent project proposal in which I was to do something very interesting with data and rather than download some data set to work with I thought it be fun to work on something new.","tags":["computer vision","r","python"],"title":"Downloading \u0026 Extracting Video Frames in R \u0026 Python","type":"post"},{"authors":null,"categories":[],"content":" Why build an image database? In a previous post I went over how to pull in video data and parse out frames for processing. That’s really useful for pulling in the data you ultimately want to screen, but at times there is also a need for having other images. You may want to classify certain objects as they appear in other videos, or build Haarcascades with specific images. Regardless you’ll need to access images. I’m going to showcase how to do this in R with imagenet.\nImagenet is a very popular image database and you can search for virtually anything. In this case I’m going to pull from the image database to construct a database from a few key word links. Ultimately, with this scenario you will easily be able to adopt the code to whatever links you want to pull.\n Define your database Lately my focus is on building object detection for drones so in my mind I figure I will need pictures of skies, different types of drones, planes and anything else a drone might run into at higher altitudes. I went ahead and searched key word:\nclick downloads:\nyou’ll want to copy the url addresses:\n Build the program Repeat this process for all the keywords you are interested in. Note some pictures may overlap in categories, this is something you will probably want to filter out later when you build out a funciton for searching duplicates. Below we start out with a loop that iterates trhough each link:\n#loop that iterates through each link library(pbapply) urls = c(#sports = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n00523513\u0026quot;, bird = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot;, drones = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03245889\u0026quot; ,bird = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot; ,people = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152\u0026quot; ,planes = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02690373\u0026quot; ,sky = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n09436708\u0026quot;) pblapply(1:length(urls),function(x){ print(urls[x]) }) ## bird ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot; ## drones ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03245889\u0026quot; ## bird ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot; ## people ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152\u0026quot; ## planes ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02690373\u0026quot; ## sky ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n09436708\u0026quot; ## [[1]] ## bird ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot; ## ## [[2]] ## drones ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03245889\u0026quot; ## ## [[3]] ## bird ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot; ## ## [[4]] ## people ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152\u0026quot; ## ## [[5]] ## planes ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02690373\u0026quot; ## ## [[6]] ## sky ## \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n09436708\u0026quot; If we loop through the urls one at a time it could take a while. In my work I often parallelize code in order to be more efficient so it’s applied here using the parallel package in R. The general write up looks like this:\ncl = makeCluster(detectCores()-1) #this function detects yours cores clusterExport(cl,c(\u0026quot;libs\u0026quot;,\u0026quot;urls\u0026quot;)) #you will want to export the functions and objects to each core clusterEvalQ(cl,lapply(libs,require,character.only = T)) #you will want to pass your libraries as well pblapply(1:length(urls),function(x){ print(urls[x]) },cl = cl) stopCluster(cl) rm(gcl) gc() You can choose to omit the parellizing if you wish, as you can see, it’s quite simple to remove or add on. Regardless now that we are successfully looping through the links we can nest another loop inside that will run through each of the links and extract the pictures. It then labels the picture by sequence and saves it into a directory. Note the trycatch function is used to pass through the errors and keep going:\n#to download negative people and sky images from imagenet #author: fausto #libraries libs = c(\u0026#39;data.table\u0026#39;, \u0026#39;lubridate\u0026#39;, \u0026#39;fasttime\u0026#39; , \u0026#39;pbapply\u0026#39;, \u0026#39;dplyr\u0026#39;, \u0026#39;parallel\u0026#39;,\u0026#39;fst\u0026#39; ,\u0026#39;RCurl\u0026#39;) lapply(libs, require, character.only = T) urls = c(#sports = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n00523513\u0026quot;, bird = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot;, drones = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03245889\u0026quot; ,bird = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01517966\u0026quot; ,people = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152\u0026quot; ,planes = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02690373\u0026quot; ,sky = \u0026quot;http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n09436708\u0026quot;) #functional loop for entire data set pull---------------------------------- #lets speed up the process with parallelizing it #stopCluster(cl) cl = makeCluster(detectCores()-3) clusterExport(cl,c(\u0026quot;libs\u0026quot;,\u0026quot;urls\u0026quot;)) clusterEvalQ(cl,lapply(libs,require,character.only = T)) pblapply(1:length(urls[1]) ,function(x){ #print the directory print(paste0(getwd(),\u0026quot;/\u0026quot;,names(urls)[x])) #create a directory for it dir.create(paste0(getwd(),\u0026quot;/\u0026quot;,names(urls)[x])) #set that directory new_direc = paste0(getwd(),\u0026quot;/\u0026quot;,names(urls)[x]) setwd(new_direc) print(new_direc) #pull the lines from the site foto = readLines(urls[x]) pblapply(1:length(foto), function(g){ #(g) #loop will continue no matter what tryCatch({ #download the data download.file( foto[g] ,destfile = paste0(new_direc ,\u0026quot;/\u0026quot;,g,\u0026quot;.jpg\u0026quot;) ,mode = \u0026quot;wb\u0026quot;) }, error=function(i){ print(\u0026quot;error keep going\u0026quot;) }) }) #reset direc setwd(\u0026quot;my output directory\u0026quot;) },cl = cl ) stopCluster(cl) rm(cl) gc() Now check you directory for the jpgs to make sure it has the files:\nsetwd(\u0026quot;my output directory\u0026quot;) length(list.files(pattern = \u0026quot;.jpg\u0026quot;)) ## [1] 1053 And read a file in to see what it looks like:\nlibrary(EBImage) setwd(direc) plot( readImage( list.files(pattern = \u0026quot;.png\u0026quot;)[4] ) ) And that’s it, you now have an image database at your disposal with seperate directories which you can manipulate to your heart’s desire. In a later post I’ll cover how to clean the bad images and have everything tidy.\n ","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"fc5657df90215a551a962554d0d6e3d5","permalink":"/post/extracting-images-from-the-internet-with-r/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/post/extracting-images-from-the-internet-with-r/","section":"post","summary":"Why build an image database? In a previous post I went over how to pull in video data and parse out frames for processing. That’s really useful for pulling in the data you ultimately want to screen, but at times there is also a need for having other images.","tags":["r","computer vision"],"title":"Extracting Images from the Internet with R","type":"post"},{"authors":null,"categories":[],"content":" Why use the NHTSA API? Recently my team has had to work with matching vehicle information to taxi data, specifically make and model information we didn’t have. It reminded me of a while back, mid 2016 during the beginnings of our driver income study when we needed vin information to calculate msrp and look at fuel consumption. At the time I had not been able to locate a package in R to call NHTSA (National Highway Transportation Authority). They have a slim but useful api.\nI hadn’t built a package before then, so I thought it would be a good chance to collect some interesting data and learn how to make a package. I reference Hilary Park’s blog at the time and created my first R package which I called caRshop. It’s nothing more than a basic wrapper but it ended up being very useful for our study as well as our enforcement unit which used the information to validate their inspection data.\n Example By now there are some powerful vin decoding packages out there but if you wish to give caRshop a chance you can install it from the repo:\ndevtools::install_github(\u0026#39;datafaust/caRshop\u0026#39;) Once it’s installed you can leverage functions to extract data for a vin. For instance you can pull for basic vin information with our favorite fast and furious character:\nlibrary(caRshop) vin_diesel(\u0026quot;1G2HX54K724118697\u0026quot;, sec = 1, tidyup = T) ## [1] \u0026quot;https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVinValues/1G2HX54K724118697?format=json\u0026quot; ## No encoding supplied: defaulting to UTF-8. ## Count Message SearchCriteria ## 1 131 Results returned successfully VIN(s): 1G2HX54K724118697 ## Results.ABS Results.ActiveSafetySysNote Results.AdaptiveCruiseControl ## 1 ## Results.AdaptiveDrivingBeam Results.AdaptiveHeadlights ## 1 ## Results.AdditionalErrorText Results.AirBagLocCurtain ## 1 ## Results.AirBagLocFront Results.AirBagLocKnee ## 1 1st Row (Driver \u0026amp; Passenger) ## Results.AirBagLocSeatCushion Results.AirBagLocSide ## 1 1st Row (Driver \u0026amp; Passenger) ## Results.Artemis Results.AutoReverseSystem ## 1 ## Results.AutomaticPedestrianAlertingSound Results.AxleConfiguration ## 1 ## Results.Axles Results.BasePrice Results.BatteryA Results.BatteryA_to ## 1 ## Results.BatteryCells Results.BatteryInfo Results.BatteryKWh ## 1 ## Results.BatteryKWh_to Results.BatteryModules Results.BatteryPacks ## 1 ## Results.BatteryType Results.BatteryV Results.BatteryV_to ## 1 ## Results.BedLengthIN Results.BedType Results.BlindSpotMon ## 1 ## Results.BodyCabType Results.BodyClass Results.BrakeSystemDesc ## 1 Sedan/Saloon ## Results.BrakeSystemType Results.BusFloorConfigType Results.BusLength ## 1 ## Results.BusType Results.CAFEBodyType Results.CAFEMake Results.CAFEModel ## 1 ## Results.CAN_AACN Results.CIB Results.CashForClunkers ## 1 ## Results.ChargerLevel Results.ChargerPowerKW Results.CoolingType ## 1 ## Results.Country Results.CurbWeightLB Results.CustomMotorcycleType ## 1 ## Results.DaytimeRunningLight Results.DestinationMarket ## 1 ## Results.DisplacementCC Results.DisplacementCI Results.DisplacementL ## 1 3800.0 231.89022755998 3.8 ## Results.Doors Results.DriveType Results.DriverAssist ## 1 4 ## Results.DynamicBrakeSupport Results.EDR Results.ESC Results.EVDriveUnit ## 1 ## Results.ElectrificationLevel Results.EngineConfiguration ## 1 V-Shaped ## Results.EngineCycles Results.EngineCylinders Results.EngineHP ## 1 6 ## Results.EngineHP_to Results.EngineKW Results.EngineManufacturer ## 1 GMPTG Flint ## Results.EngineModel Results.EntertainmentSystem Results.EquipmentType ## 1 L36 ## Results.ErrorCode ## 1 0 - VIN decoded clean. Check Digit (9th position) is correct ## Results.ForwardCollisionWarning Results.FuelInjectionType ## 1 Sequential Fuel Injection (SFI) ## Results.FuelTypePrimary Results.FuelTypeSecondary Results.GVWR ## 1 ## Results.KeylessIgnition Results.LaneDepartureWarning ## 1 ## Results.LaneKeepSystem Results.LowerBeamHeadlampLightSource Results.Make ## 1 PONTIAC ## Results.Manufacturer Results.ManufacturerId Results.ManufacturerType ## 1 GENERAL MOTORS LLC 984 ## Results.Model Results.ModelYear Results.MotorcycleChassisType ## 1 Bonneville 2002 ## Results.MotorcycleSuspensionType Results.NCAPBodyType Results.NCAPMake ## 1 ## Results.NCAPModel Results.NCICCode Results.NCSABodyType Results.NCSAMake ## 1 ## Results.NCSAModel ## 1 ## Results.Note ## 1 Body Type: Sedan, 4-6 Window, Notchback (GM codes: 19, 69) ## Results.OtherBusInfo Results.OtherEngineInfo ## 1 Name Plate: Chevrolet, Pontiac, Buick ## Results.OtherMotorcycleInfo Results.OtherRestraintSystemInfo ## 1 ## Results.OtherTrailerInfo Results.ParkAssist ## 1 ## Results.PedestrianAutomaticEmergencyBraking Results.PlantCity ## 1 Orion ## Results.PlantCompanyName Results.PlantCountry Results.PlantState ## 1 NA-GM Corp United States (USA) Michigan ## Results.PossibleValues Results.Pretensioner ## 1 ## Results.RearCrossTrafficAlert Results.RearVisibilitySystem ## 1 ## Results.SAEAutomationLevel Results.SAEAutomationLevel_to ## 1 ## Results.SeatBeltsAll Results.SeatRows Results.Seats ## 1 Manual ## Results.SemiautomaticHeadlampBeamSwitching Results.Series ## 1 SE ## Results.Series2 Results.SteeringLocation Results.SuggestedVIN ## 1 ## Results.TPMS Results.TopSpeedMPH Results.TrackWidth ## 1 ## Results.TractionControl Results.TrailerBodyType Results.TrailerLength ## 1 ## Results.TrailerType Results.TransmissionSpeeds Results.TransmissionStyle ## 1 ## Results.Trim Results.Trim2 Results.Turbo Results.VIN ## 1 1G2HX54K724118697 ## Results.ValveTrainDesign Results.VehicleType Results.WheelBaseLong ## 1 Overhead Valve (OHV) PASSENGER CAR ## Results.WheelBaseShort Results.WheelBaseType Results.WheelSizeFront ## 1 ## Results.WheelSizeRear Results.Wheels Results.Windows ## 1 4 You’ll see that this prints out the entire record response fromt the json, with a few parameters to go ahead and convert the response to a dataframe as well as not overload the api with too many calls too quickly.\nIn our case we needed to loop over thousands of vehicles. As an example of what a loop for that might look like here we run a few (I’m a huge fan of the pbapply package):\nlibrary(pbapply) library(dplyr) library(data.table) my_cars = c(\u0026#39;1G2HX54K724118697\u0026#39;, \u0026#39;5UXZV4C58D0E07160\u0026#39;, \u0026#39;1NXBR18E1XZ184142\u0026#39;) #run loop my_data = my_cars %\u0026gt;% pblapply(function(x){ vin_diesel(x, sec = 1, tidyup = T) }) %\u0026gt;% rbindlist() ## [1] \u0026quot;https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVinValues/1G2HX54K724118697?format=json\u0026quot; ## [1] \u0026quot;https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVinValues/5UXZV4C58D0E07160?format=json\u0026quot; ## [1] \u0026quot;https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVinValues/1NXBR18E1XZ184142?format=json\u0026quot; print(my_data[,.(Results.Make, Results.Model)]) ## Results.Make Results.Model ## 1: PONTIAC Bonneville ## 2: BMW X5 ## 3: TOYOTA Corolla I went ahead and pulled only a few columns, but you get the idea. Running in Python is pretty straightforward, but I’ll go over that another time. This was a fun experience and a great lookback to a time when I was still learning some of the more basic functions in R.\n ","date":1507939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507939200,"objectID":"5c925b9b179739717dd2ffda80a9f54b","permalink":"/post/making-an-r-package-to-call-nhtsa-api/","publishdate":"2017-10-14T00:00:00Z","relpermalink":"/post/making-an-r-package-to-call-nhtsa-api/","section":"post","summary":"Why use the NHTSA API? Recently my team has had to work with matching vehicle information to taxi data, specifically make and model information we didn’t have. It reminded me of a while back, mid 2016 during the beginnings of our driver income study when we needed vin information to calculate msrp and look at fuel consumption.","tags":["r","transportation","taxi"],"title":"Making an R package to Call NHTSA API","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":"A Self Conscious Printer Since it\u0026rsquo;s the middle of the summer and I\u0026rsquo;m out and about, I don\u0026rsquo;t get as much time to play with my new toys, too busy hiking and motorcycling. Finally got a chance to work on my 3d printer again. I though it best to set up some sort of home server that I could use to track my printer, send it jobs from elsewhere and generally control it remotely.\nA lot of people online were recommending Octopi or Astrobox as good options to run a server, but both required a raspberrypi 3 which I didn\u0026rsquo;t have. What I did have lying around were some raspberry pi zeros so I figured I could make do with those. I decided to go with this lovely model.\n","date":1501214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501214400,"objectID":"c7c64876bc3e7dc4a2de7aaade92a262","permalink":"/post/rpi_cam/","publishdate":"2017-07-28T00:00:00-04:00","relpermalink":"/post/rpi_cam/","section":"post","summary":"A look at how to make a quick raspberrypi zero server for your 3d printer.","tags":["Academic"],"title":"3d Printing Parts for my Printer: Raspberry Pi Camera Assembly","type":"post"},{"authors":["Fausto"],"categories":null,"content":"My 3d printer finally arrived in the mail, and I spend the better half of my day assembling it and tuning it to get it to print something that didn\u0026rsquo;t look like a mess. After a bunch of trial and error I finally got it running. Check the video here.\n","date":1497931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497931200,"objectID":"c9cd68299e56edb8a355cbb4bfe808a9","permalink":"/post/threed_printer_running/","publishdate":"2017-06-20T00:00:00-04:00","relpermalink":"/post/threed_printer_running/","section":"post","summary":"A short post on my 3d printer.","tags":["Academic"],"title":"My 3d Printer is Running!","type":"post"},{"authors":["Fausto Lopez"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   -- ","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/talk/example/","section":"talk","summary":"A fun talk with Open Data staff at Google's Civi Hall offices, I had a chance to present some of the work we do at TLC, interesting trends and fun projects. You can watch the video of the presentation [here](https://www.facebook.com/CivicHallNYC/videos/2042845915785394/).","tags":[],"title":"Open Data Talk: TLC and Taxi Data At Civic Hall","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"5a534cd00f6495714298d359c0db5a92","permalink":"/project/dblpark/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/dblpark/","section":"project","summary":"DoublePark helps you notify someone when you are double parked. This mobile app was written using MySQL, NodeJs, and React-Native.","tags":["react"],"title":"A DOUBLE PARKING APP IN REACT","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"f67814db3f0bacfbe149f9c9161eac31","permalink":"/project/pooper/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/pooper/","section":"project","summary":"Pooper is a fun prototype web app optimized for mobile use. It is a full-stack app with backend written in R, SQL, and NodeJS, and frontend in ReactJS.","tags":["react"],"title":"A POOPER APP IN REACT","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"f7397d1e0158418f05d7871310a24d73","permalink":"/project/citibike/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/citibike/","section":"project","summary":"Built in React JS and optimized for mobile, this webapp helps you access citibike station live feed.","tags":["Demo"],"title":"Citibike Locator: find citi-bike locations in NYC","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/external-project/","section":"project","summary":"Built in React JS and optimized for mobile, this webapp was a fun react project to get started with.","tags":["Demo"],"title":"LinkNYC Locator: find Wi-fi spots","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"f2fac2d711d9b790ac0b7df932d18418","permalink":"/project/platenet/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/platenet/","section":"project","summary":"Built in React-Native, this mobile app will allow you to scan your plate and see your tickets and violations.","tags":["react"],"title":"PLATENET MOBILE APP (Coming Soon...)","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"108f17767e543e319346ed9f586767e3","permalink":"/project/icarus/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/icarus/","section":"project","summary":"Realtime flight route database using dockerized Python, R and MySql.","tags":[""],"title":"Project Icarus: a realtime flight route database using OpenRadar","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"96965533b67d77877f6eee1553e772da","permalink":"/project/rodents/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/rodents/","section":"project","summary":"Built in React JS and optimized for mobile, this webapp tracks metrics for rat inspections and showcases a live search from open data.","tags":["react"],"title":"RatInspector: a webapp tracking NYC's battle against the rat hordes","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"a98234a4ad24223bfc6cc212380022e9","permalink":"/project/taxifastdash/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/taxifastdash/","section":"project","summary":"Built using Shiny from Rstudio, this dashboard expands on the taxi industry indicators report published by the Taxi \u0026 Limousine Commission.","tags":["react"],"title":"Taxi Fast Dashboard","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"1931ec94a03b3b337d4991ddaa46c2b3","permalink":"/project/tlcdatahub/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/tlcdatahub/","section":"project","summary":"A collaboration with Nikita Voevodin, the TLC data Hub was built with R Shiny and published on behalf of the Taxi \u0026 Limousine Commission.","tags":["react"],"title":"TLC DATA HUB","type":"project"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"ada9e2c50d0265cf46e85aa8df13492e","permalink":"/project/verycoolstudios/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/verycoolstudios/","section":"project","summary":"Built in React JS in collaboration with Nikita Voevodin, this site offers information on our consulting service.","tags":["react"],"title":"VERYCOOL STUDIO WEBSITE","type":"project"},{"authors":[],"categories":null,"content":"","date":1461124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515819600,"objectID":"42b8929c0d73b802ada0d63991329a9e","permalink":"/post/sonar_step1/","publishdate":"2016-04-20T00:00:00-04:00","relpermalink":"/post/sonar_step1/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"blah blah blah","type":"post"},{"authors":["Fausto Lopez","Veronica Momjian","Paula Bonfatti"],"categories":null,"content":"","date":1427860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427860800,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-04-01T00:00:00-04:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Urban Upbound's Healthy Aging in Far Rockaway report was developed under the New York Women's Foundation Healthy Aging Initiative, a push for evidence-based program planning for aging women in New York City. We conducted qualitative and quantitative research throughout different New York City Housing Authority Houses in Far Rockaway New York to see how senior women were fairing.","tags":[],"title":"Healthy Aging in Far Rockaway","type":"publication"}]